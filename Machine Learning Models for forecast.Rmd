---
title: "HN"
author: "Fahima Raduana"
date: "9/18/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Rlof)
library(dplyr)
library(skimr)
library(DMwR2)
library("readxl") # used to read excel files
library("dplyr") # used for data munging 
library("FNN") # used for knn regression (knn.reg function)
library("caret") # used for various predictive models
library("class") # for using confusion matrix function
library("rpart.plot") # used to plot decision tree
library("rpart")  # used for Regression tree
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("PRROC") # top plot ROC curve
library("ROCR") # top plot lift curve
library("tidyverse")
library("e1071")
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
library("randomForest")
library(pROC)
library(mlbench)
```


```{r}

# Load the training data

#read the CSV file into a data frame 'HN_data'
HN_data <- read.csv("HN_data_PostModule.csv")
#HN_data$adopter <- as.factor(HN_data$adopter)
# lets look at all the variables
skim(HN_data)
#str(HN_data)
#glimpse(HN_data)
#View(HN_data_PostModule)
```


```{r}
# create Y and X data frames
# I pulled the y below
#HN_data_y = HN_data %>% pull("adopter")
# exclude net_user since its a row number
HN_data_x = HN_data %>% select(-c("net_user"))

#to change all the na values to average for all the columns
for(i in 1:ncol(HN_data_x)){
  HN_data_x[is.na(HN_data_x[,i]), i] <- mean(HN_data_x[,i], na.rm = TRUE)
}
#Replacing na to average for age, needed to replace it seperately because in above
#code I did a for loop from column 3 to the rest of the rightside and couldn't add #age in the loop
#HN_data_x$age[is.na(HN_data_x$age)] <- mean(HN_data_x$age,na.rm = TRUE)
#Replacing NA with "UNK" for binary variable "male"
#HN_data_x <-  HN_data_x %>%
#  mutate(male = ifelse(is.na(male),"UNK",male))
#to change all the na values to "UNK" for all the columns
#for(i in 0(HN_data_x)){
#  HN_data_x[is.na(HN_data_x[,i]), i] <- ("UNK")
#  }
#HN_data_x[] <- lapply(HN_data_x, factor)
skim(HN_data_x)
#glimpse(HN_data_x)

```

```{r}
HN_data_x$adopter <- as.factor(HN_data_x$adopter)
HN_data_delta = HN_data_x %>% select(c("age","male","tenure","delta1_friend_cnt","delta1_avg_friend_age" ,"delta1_avg_friend_male","delta1_friend_country_cnt","delta1_subscriber_friend_cnt","delta1_songsListened","delta1_lovedTracks","delta1_posts","delta1_playlists","delta1_shouts","delta1_good_country","adopter"))

```


# Split delta

```{r}

# 75% of the data is used for training and rest for testing including adopter so that during random selection the consistency stays
smp_size <- floor(0.75 * nrow(HN_data_delta))


# randomly select row numbers for training data set including adopter
train_ind <- sample(seq_len(nrow(HN_data_delta)), size = smp_size)

#creating vector for y which is adopter
HN_delta_y = HN_data_delta %>% pull("adopter")
HN_data_delta = HN_data_delta %>% select(-c("adopter"))

# creating test and training sets for x
HN_delta_x_train <- HN_data_delta[train_ind, ]
HN_delta_x_test <- HN_data_delta[-train_ind, ]


# creating test and training sets for y
HN_delta_y_train <- HN_delta_y[train_ind]
HN_delta_y_test <- HN_delta_y[-train_ind]

# Create an empty data frame to store results from different models
clf_results <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results) <- c("Model", "Accuracy", "Precision", "Recall", "F1")

# Create an empty data frame to store TP, TN, FP and FN values
cost_benefit_df <- data.frame(matrix(ncol = 5, nrow = 0))
names(cost_benefit_df) <- c("Model", "TP", "FN", "FP", "TN")

```

#XGBoost delta

```{r}
#
cross_validation <- trainControl(## 4-fold CV
                                method = "repeatedcv",
                                number = 4,
                                ## repeated three times
                                repeats = 1)
Param_Grid <-  expand.grid(nrounds = 1000 , max_depth = 20,subsample=0.5,colsample_bytree=0.6,min_child_weight=1,
                           eta = 0.01,gamma=0)

```



```{r }
XG_clf_fit <- train(HN_delta_x_train, 
                    HN_delta_y_train,
                    method = "xgbTree",
                    tuneGrid = Param_Grid,
                    trControl = cross_validation)
```

```{r }
# print the final model
XG_clf_fit$finalModel
```

```{r }
# Predict on test data
XG_clf_predict <- predict(XG_clf_fit,HN_delta_x_test)
```


```{r }
# Print Confusion matrix, Accuracy, Sensitivity etc 
confusionMatrix(XG_clf_predict,  HN_delta_y_test, positive = "1" )

# Add results into clf_results dataframe
x4 <- confusionMatrix(XG_clf_predict,  HN_delta_y_test )[["overall"]]
y4 <- confusionMatrix(XG_clf_predict,  HN_delta_y_test )[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "XG Boost", 
                                             Accuracy = round (x4[["Accuracy"]],3), 
                                            Precision = round (y4[["Precision"]],3), 
                                            Recall = round (y4[["Recall"]],3), 
                                            F1 = round (y4[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x4[["Accuracy"]],3), "and F1 is ", round (y4[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a4 <- confusionMatrix(XG_clf_predict,  HN_delta_y_test )


```

```{r}
HN_data_target = filter(HN_data_x, adopter == 1)
HN_data_target_x = HN_data_target %>% select(-c("adopter"))
HN_data_target_x_dummy = dummy.data.frame(data.frame(HN_data_target_x), sep = "-")
xgb_all = predict(XG_clf_fit, newdata = HN_data_target_x_dummy, type = "prob")
HN_data_target$ProbConversion = xgb_all[,2]
HN_data_target = HN_data_target %>% arrange(desc(ProbConversion))
SuccessNum = HN_data_target[1:1000,] %>% filter(ProbConversion>.5) %>% tally()
cat(SuccessNum[[1]])
HN_data_target_1000 = HN_data_target [1:1000,]
#View(HN_data_target_1000)

write.csv(x = HN_data_target_1000, file = "HN target 1000 xgb.csv", row.names = TRUE)
write.csv(x = HN_data_target, file = "HN target xgb.csv", row.names = TRUE)

```

## 1.6 Neural Network classification delta

```{r message=FALSE,  warning=FALSE }

# Try different combinations of parameters like 
# decay (prevents the weights from growing too large,) 
# and size of Hidden layers
my.grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 7))

# stepmax is maximum steps for the training of the neural network
# threshold is set to 0.01, meaning that if the change in error during an iteration is 
# less than 1%, then no further optimization will be carried out by the model
nn_clf_fit <- train(HN_delta_x_train,
                    HN_delta_y_train,
                    method = "nnet",
                    trace = F,
                    tuneGrid = my.grid,
                    linout = 0,
                    stepmax = 100,
                    threshold = 0.01 )
print(nn_clf_fit)


# Plot Neural Network 
plotnet(nn_clf_fit$finalModel, y_names = "adopter")

```

```{r }
# Predict on test data
nn_clf_predict <- predict(nn_clf_fit,HN_delta_x_test)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(nn_clf_predict,  HN_delta_y_test, positive = "1")

# Add results into clf_results dataframe
x5 <- confusionMatrix(nn_clf_predict,  HN_delta_y_test)[["overall"]]
y5 <- confusionMatrix(nn_clf_predict,  HN_delta_y_test)[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Neural Network", 
                                             Accuracy = round (x5[["Accuracy"]],3), 
                                            Precision = round (y5[["Precision"]],3), 
                                            Recall = round (y5[["Recall"]],3), 
                                            F1 = round (y5[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x5[["Accuracy"]],3), "and F1 is ", round (y5[["F1"]],3)  )


# Add results into cost_benefit_df dataframe for cost benefit analysis 
a5 <- confusionMatrix(nn_clf_predict,  HN_delta_y_test)

```

```{r}
HN_data_target = filter(HN_data_x, adopter == 1)
HN_data_target_x = HN_data_target %>% select(-c("adopter"))
HN_data_target_x_dummy = dummy.data.frame(data.frame(HN_data_target_x), sep = "-")
xgb_all = predict(nn_clf_fit, newdata = HN_data_target_x_dummy, type = "prob")
HN_data_target$ProbConversion = xgb_all[,2]
HN_data_target = HN_data_target %>% arrange(desc(ProbConversion))
SuccessNum = HN_data_target[1:1000,] %>% filter(ProbConversion>.5) %>% tally()
cat(SuccessNum[[1]])
HN_data_target_1000 = HN_data_target [1:1000,]
#View(HN_data_target_1000)

write.csv(x = HN_data_target_1000, file = "HN target 1000 nn.csv", row.names = TRUE)
write.csv(x = HN_data_target, file = "HN target nn.csv", row.names = TRUE)

```


#GLM delta

```{r }
HN_delta_y_train_l <- ifelse(HN_delta_y_train =="1", 1,0)
HN_delta_y_test_l <- ifelse(HN_delta_y_test =="0",0, 1)
```

```{r  message=FALSE,  warning=FALSE}
glm_fit <- train(HN_delta_x_train,
                 HN_delta_y_train_l, 
                 method = "glm",
                 family = "binomial",
                 preProc = c("center", "scale"))
```

```{r }
# Predict on test data
glm_predict <- predict(glm_fit, newdata = HN_delta_x_test)

```

convert probability outcome into categorical outcome 
```{r }
y_pred_num <- ifelse(glm_predict > .9, 1, 0)

```


```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(as.factor(y_pred_num), as.factor(HN_delta_y_test), positive = "1")

# Add results into clf_results dataframe
x3 <- confusionMatrix(as.factor(y_pred_num), as.factor(HN_delta_y_test), positive = "1")[["overall"]]
y3 <- confusionMatrix(as.factor(y_pred_num), as.factor(HN_delta_y_test),positive = "1")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Logistic Regression", 
                                             Accuracy = round (x3[["Accuracy"]],3), 
                                            Precision = round (y3[["Precision"]],3), 
                                            Recall = round (y3[["Recall"]],3), 
                                            F1 = round (y3[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x3[["Accuracy"]],3), "and F1 is ", round (y3[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a3 <- confusionMatrix(as.factor(y_pred_num), as.factor(HN_data_y_test))

#be careful about accurately pickign up the TP, FN, FP and TN
cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Logistic Regression", 
                                             TP = a3[["table"]][4], 
                                             FN = a3[["table"]][3], 
                                             FP = a3[["table"]][2], 
                                             TN = a3[["table"]][1])
```

```{r }

print(clf_results)


```

```{r}
XG_boost_prob <- predict(XG_clf_fit, newdata = HN_delta_x_test, type = "prob")

```



# AUC delta

```{r}

# Predict probabilities of each model to plot ROC curve
#knnPredict_prob <- predict(knn_clf_fit, newdata = cancer_x_test, type = "prob") 
#gml_predict <- predict(_fit, newdata = HN_data_x_test, type = "prob")
XG_boost_prob <- predict(XG_clf_fit, newdata = HN_delta_x_test, type = "prob")
nn_clf_prob <- predict(nn_clf_fit, newdata = HN_delta_x_test, type = "prob")

# List of predictions
preds_list <- list(glm_predict, XG_boost_prob[,1], nn_clf_prob[,1] )

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(HN_delta_y_test_l), m)

# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")

# calculate AUC for all models
AUC_models <- performance(pred, "auc")
auc_lm = round(AUC_models@y.values[[1]], 3)
auc_xg = round(AUC_models@y.values[[2]], 3)
auc_nn = round(AUC_models@y.values[[3]], 3)

# Plot the ROC curves
plot(rocs, col = as.list(1:m), main = "ROC Curves of different models")
legend(x = "bottomright", 
       legend = c( paste0("Glm - ", auc_lm), 
                   paste0("XG Boost - ", auc_xg), 
                   paste0("Neural Net - ", auc_nn)), fill = 1:m)

```

**Lift curve** - Lift is a measure of the effectiveness of a predictive model calculated as the ratio between the results obtained with and without the predictive model. The lift chart shows how much more likely we are to predict the correct outcome than a random guess.


```{r}

lifts <- performance(pred, "lift", "rpp")

# Plot the Lift curves
plot(lifts, col = as.list(1:m), main = "Lift Curves of Different Models")
legend(x = "bottomleft", 
       legend = c( "Logistic Regression", 
                   "XG Boost", 
                   "Neural Net"), fill = 2:m)


```
#Current data

```{r}
HN_data_x$adopter <- as.factor(HN_data_x$adopter)
HN_data_reg = HN_data_x %>% select(c("age","male","tenure","friend_cnt","avg_friend_age" ,"avg_friend_male","friend_country_cnt","subscriber_friend_cnt","songsListened","lovedTracks","posts","playlists","shouts","good_country","adopter"))

```

# Split current data 
```{r}

# 75% of the data is used for training and rest for testing including adopter so that during random selection the consistency stays
smp_size <- floor(0.75 * nrow(HN_data_reg))


# randomly select row numbers for training data set including adopter
train_ind <- sample(seq_len(nrow(HN_data_reg)), size = smp_size)

#creating vector for y which is adopter
HN_reg_y = HN_data_reg %>% pull("adopter")
HN_data_reg = HN_data_reg %>% select(-c("adopter"))

# creating test and training sets for x
HN_reg_x_train <- HN_data_reg[train_ind, ]
HN_reg_x_test <- HN_data_reg[-train_ind, ]


# creating test and training sets for y
HN_reg_y_train <- HN_reg_y[train_ind]
HN_reg_y_test <- HN_reg_y[-train_ind]

# Create an empty data frame to store results from different models
clf_results_reg <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results_reg) <- c("Model", "Accuracy", "Precision", "Recall", "F1")

# Create an empty data frame to store TP, TN, FP and FN values
cost_benefit_df <- data.frame(matrix(ncol = 5, nrow = 0))
names(cost_benefit_df) <- c("Model", "TP", "FN", "FP", "TN")

```


#XGBoost current data 

```{r}
#
cross_validation <- trainControl(## 4-fold CV
                                method = "repeatedcv",
                                number = 4,
                                ## repeated three times
                                repeats = 1)
Param_Grid <-  expand.grid(nrounds = 1000 , max_depth = 20,subsample=0.5,colsample_bytree=0.6,min_child_weight=1,
                           eta = 0.02,gamma=0)

```



```{r }
XG_clf_fit_reg <- train(HN_reg_x_train, 
                    HN_reg_y_train,
                    method = "xgbTree",
                    tuneGrid = Param_Grid,
                    trControl = cross_validation)
```

```{r }
# print the final model
XG_clf_fit_reg$finalModel
```

```{r }
# Predict on test data
XG_clf_predict_reg <- predict(XG_clf_fit_reg,HN_reg_x_test)
```


```{r }
# Print Confusion matrix, Accuracy, Sensitivity etc 
confusionMatrix(XG_clf_predict_reg,  HN_reg_y_test, positive = "1" )

# Add results into clf_results dataframe
x4 <- confusionMatrix(XG_clf_predict_reg,  HN_reg_y_test )[["overall"]]
y4 <- confusionMatrix(XG_clf_predict_reg,  HN_reg_y_test )[["byClass"]]

clf_results_reg[nrow(clf_results_reg) + 1,] <-  list(Model = "XG Boost", 
                                             Accuracy = round (x4[["Accuracy"]],3), 
                                            Precision = round (y4[["Precision"]],3), 
                                            Recall = round (y4[["Recall"]],3), 
                                            F1 = round (y4[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x4[["Accuracy"]],3), "and F1 is ", round (y4[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a4 <- confusionMatrix(XG_clf_predict_reg,  HN_reg_y_test )

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "XG Boost", 
                                             TP = a4[["table"]][1], 
                                             FN = a4[["table"]][2], 
                                             FP = a4[["table"]][3], 
                                             TN = a4[["table"]][4])

```

```{r}
XG_boost_prob_reg <- predict(XG_clf_fit_reg, newdata = HN_reg_x_test, type = "prob")
#prob_top1000<- df[order(XG_boost_prob_reg$`1`,decreasing=TRUE),][1:1000,]
#top1000<-cbind.data.frame(XG_boost_prob_reg == 1 >.9)
#print(length(top1000 == 1))

#y_pred_xb1 <- ifelse(XG_boost_prob_reg < 0.9, 1,0)
#print(length(y_pred_xb1))
```

## 1.6 Neural Network classification current data 

```{r message=FALSE,  warning=FALSE }

# Try different combinations of parameters like 
# decay (prevents the weights from growing too large,) 
# and size of Hidden layers
my.grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 7))

# stepmax is maximum steps for the training of the neural network
# threshold is set to 0.01, meaning that if the change in error during an iteration is 
# less than 1%, then no further optimization will be carried out by the model
nn_clf_fit_reg <- train(HN_reg_x_train,
                    HN_reg_y_train,
                    method = "nnet",
                    trace = F,
                    tuneGrid = my.grid,
                    linout = 0,
                    stepmax = 100,
                    threshold = 0.01 )
print(nn_clf_fit_reg)


# Plot Neural Network 
plotnet(nn_clf_fit_reg$finalModel, y_names = "adopter")

```

```{r }
# Predict on test data
nn_clf_predict_reg <- predict(nn_clf_fit_reg,HN_reg_x_test)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(nn_clf_predict_reg,  HN_reg_y_test, positive = "1")

# Add results into clf_results dataframe
x5 <- confusionMatrix(nn_clf_predict_reg,  HN_reg_y_test)[["overall"]]
y5 <- confusionMatrix(nn_clf_predict_reg,  HN_reg_y_test)[["byClass"]]

clf_results_reg[nrow(clf_results_reg) + 1,] <-  list(Model = "Neural Network", 
                                             Accuracy = round (x5[["Accuracy"]],3), 
                                            Precision = round (y5[["Precision"]],3), 
                                            Recall = round (y5[["Recall"]],3), 
                                            F1 = round (y5[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x5[["Accuracy"]],3), "and F1 is ", round (y5[["F1"]],3)  )


# Add results into cost_benefit_df dataframe for cost benefit analysis 
a5 <- confusionMatrix(nn_clf_predict,  HN_reg_y_test)

cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Neural Network", 
                                             TP = a5[["table"]][1], 
                                             FN = a5[["table"]][2], 
                                             FP = a5[["table"]][3], 
                                             TN = a5[["table"]][4])

```


```{r}

nn_clf_prob_reg <- predict(nn_clf_fit_reg, newdata =head(HN_reg_x_test, decreasing = TRUE)[1:1000,], type = "prob")
#logmodel <- function(nn_clf_prob_reg,HN_reg_x_test,HN_reg_y_test,nn_clf_fit_reg{ 
#  df = data.frame(nn_clf_prob_reg)
#  df$adopter = HN_reg_y_test
#  prob_top1000<- df[order(nn_clf_prob_reg,decreasing=TRUE),][1:1000,]
#  prob_top1000$nn_clf_prob_reg <- ifelse(prob_top1000$model_prob > 0, 1, 0

#top1000<-cbind.data.frame(nn_clf_prob_reg == 0, decreasing = FALSE)
#print(length(top1000))
```

#GML current data 

```{r }
HN_reg_y_train_l <- ifelse(HN_reg_y_train =="1", 1,0)
HN_reg_y_test_l <- ifelse(HN_reg_y_test =="0",0, 1)
```

```{r  message=FALSE,  warning=FALSE}
glm_fit_reg <- train(HN_reg_x_train,
                 HN_reg_y_train_l, 
                 method = "glm",
                 family = "binomial",
                 preProc = c("center", "scale"))
```

```{r }
# Predict on test data
glm_predict_reg <- predict(glm_fit_reg, newdata = HN_reg_x_test)

```

convert probability outcome into categorical outcome 
```{r }
y_pred_num_reg <- ifelse(glm_predict_reg > .2, 1, 0)

```


```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(as.factor(y_pred_num_reg), as.factor(HN_reg_y_test), positive = "1")

# Add results into clf_results dataframe
x3 <- confusionMatrix(as.factor(y_pred_num_reg), as.factor(HN_reg_y_test), positive = "1")[["overall"]]
y3 <- confusionMatrix(as.factor(y_pred_num_reg), as.factor(HN_reg_y_test),positive = "1")[["byClass"]]

clf_results_reg[nrow(clf_results_reg) + 1,] <-  list(Model = "Logistic Regression", 
                                             Accuracy = round (x3[["Accuracy"]],3), 
                                            Precision = round (y3[["Precision"]],3), 
                                            Recall = round (y3[["Recall"]],3), 
                                            F1 = round (y3[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x3[["Accuracy"]],3), "and F1 is ", round (y3[["F1"]],3)  )

# Add results into cost_benefit_df dataframe for cost benefit analysis 
a3 <- confusionMatrix(as.factor(y_pred_num_reg), as.factor(HN_reg_y_test))

#be careful about accurately pickign up the TP, FN, FP and TN
cost_benefit_df[nrow(cost_benefit_df) + 1,] <-  list(Model = "Logistic Regression", 
                                             TP = a3[["table"]][4], 
                                             FN = a3[["table"]][3], 
                                             FP = a3[["table"]][2], 
                                             TN = a3[["table"]][1])
```


```{r}

# Predict probabilities of each model to plot ROC curve
#knnPredict_prob <- predict(knn_clf_fit, newdata = cancer_x_test, type = "prob") 

XG_boost_prob_reg <- predict(XG_clf_fit_reg, newdata = HN_reg_x_test, type = "prob")

nn_clf_prob_reg <- predict(nn_clf_fit_reg, newdata = HN_reg_x_test, type = "prob")

# List of predictions
preds_list <- list(glm_predict_reg, XG_boost_prob_reg[,1], nn_clf_prob_reg[,1] )
preds_list
#prob_top1000<- df[order(preds_list$,decreasing=TRUE),][1:1000,]]
# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(HN_reg_y_test_l), m)


# Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")

# calculate AUC for all models
AUC_models <- performance(pred, "auc")
auc_lm = round(AUC_models@y.values[[1]], 3)
auc_xg = round(AUC_models@y.values[[2]], 3)
auc_nn = round(AUC_models@y.values[[3]], 3)

# Plot the ROC curves
plot(rocs, col = as.list(1:m), main = "ROC Curves of different models")
legend(x = "bottomright", 
       legend = c( paste0("Glm - ", auc_lm), 
                   paste0("XG Boost - ", auc_xg), 
                   paste0("Neural Net - ", auc_nn)), fill = 1:m)

```

```{r}

lifts <- performance(pred, "lift", "rpp")

# Plot the Lift curves
plot(lifts, col = as.list(1:m), main = "Lift Curves of Different Models")
legend(x = "bottomleft", 
       legend = c( "Logistic Regression", 
                   "XG Boost", 
                   "Neural Net"), fill = 2:m)


```


```{r}
HN_data_target = filter(HN_data, adopter == 0)
HN_data_target_x = HN_data_target %>% select(-c("adopter"))
HN_data_target_x_dummy = dummy.data.frame(data.frame(HN_data_target_x), sep = "-")
xgb_all = predict(XG_clf_fit_delta, newdata = HN_data_target_x_dummy, type = "prob")
HN_data_target$ProbConversion = xgb_all[,2]
HN_data_target = HN_data_target %>% arrange(desc(ProbConversion))
SuccessNum = HN_data_target[1:1000,] %>% filter(ProbConversion>.5) %>% tally()
cat(SuccessNum[[1]])
HN_data_target_1000 = HN_data_target [1:1000,]
#View(HN_data_target_1000)

write.csv(x = HN_data_target_1000, file = "HN target 1000 xgb.csv", row.names = TRUE)
write.csv(x = HN_data_target, file = "HN target xgb.csv", row.names = TRUE)

```


